{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b89d3a0-b106-43d2-9bfd-fefad16b57df",
   "metadata": {},
   "source": [
    "# <center>ChatBot \"Python\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c4faa-4859-4325-b295-99b3ea925846",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e60338a-1e0c-459a-b73e-5ddbf8a68a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, re, random, hashlib, json, codecs, os, io, textwrap, copy, zipfile\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0021ca-2720-45b6-b9ef-ef4271af1ed6",
   "metadata": {},
   "source": [
    "## Build Product PyPi database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda25e7a-16e6-46dc-88b0-280e6004b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_package_names(url, max_n = None):\n",
    "    \"\"\"Extract package names from stats page.\"\"\"\n",
    "\n",
    "    html = requests.get(url, timeout = 20).text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    names = []\n",
    "    for a in soup.select(\"a[href*='pypi.org/project/']\"):\n",
    "        name = (a.get_text() or \"\").strip()\n",
    "        if name and re.fullmatch(r\"[A-Za-z0-9][A-Za-z0-9._\\-]*\", name):\n",
    "            names.append(name)\n",
    "\n",
    "    # Duplicate preserving order\n",
    "    seen = set(); uniq = []\n",
    "    for n in names:\n",
    "        if n not in seen:\n",
    "            uniq.append(n); seen.add(n)\n",
    "    if max_n is not None:\n",
    "        uniq = uniq[:int(max_n)]\n",
    "    return uniq\n",
    "\n",
    "def strip_html(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    try:\n",
    "        return BeautifulSoup(s, 'lxml').get_text(\" \", strip = True)\n",
    "    except Exception:\n",
    "        # Fallback: crude tag strip\n",
    "        return re.sub(r\"<[^>]+>\", \" \", s)\n",
    "\n",
    "def fetch_pypi_pkg(name: str):\n",
    "    \"\"\"Fetch title / description from PyPI JSON API.\"\"\"\n",
    "\n",
    "    url = f\"https://pypi.org/pypi/{name}/json\"\n",
    "    r = requests.get(url, timeout = 20)\n",
    "    r.raise_for_status()\n",
    "    info = r.json().get('info', {})\n",
    "    title = info.get(\"name\", name) or name\n",
    "    desc = info.get('summary') or info.get('description') or \"\"\n",
    "    return {\n",
    "        'product_id' : name,\n",
    "        'title' : str(title),\n",
    "        'description' : strip_html(desc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77216bd-f9d3-42be-b094-31f6d07f5e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1062 package names_internet on the page.\n",
      "skip office365-rest-client -> 404 Client Error: Not Found for url: https://pypi.org/pypi/office365-rest-client/json\n",
      "skip turtle -> 404 Client Error: Not Found for url: https://pypi.org/pypi/turtle/json\n",
      "skip flask-bootstrapforms -> 404 Client Error: Not Found for url: https://pypi.org/pypi/flask-bootstrapforms/json\n",
      "skip django-toolbelt -> 404 Client Error: Not Found for url: https://pypi.org/pypi/django-toolbelt/json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urllib3</td>\n",
       "      <td>urllib3</td>\n",
       "      <td>HTTP library with thread-safe connection pooli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>requests</td>\n",
       "      <td>requests</td>\n",
       "      <td>Python HTTP for Humans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idna</td>\n",
       "      <td>idna</td>\n",
       "      <td>Internationalized Domain Names in Applications...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google-api-core</td>\n",
       "      <td>google-api-core</td>\n",
       "      <td>Google API client core library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google-auth</td>\n",
       "      <td>google-auth</td>\n",
       "      <td>Google Authentication Library</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id            title  \\\n",
       "0          urllib3          urllib3   \n",
       "1         requests         requests   \n",
       "2             idna             idna   \n",
       "3  google-api-core  google-api-core   \n",
       "4      google-auth      google-auth   \n",
       "\n",
       "                                         description  \n",
       "0  HTTP library with thread-safe connection pooli...  \n",
       "1                            Python HTTP for Humans.  \n",
       "2  Internationalized Domain Names in Applications...  \n",
       "3                     Google API client core library  \n",
       "4                      Google Authentication Library  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED_URL_internet = 'https://n0x5.github.io/PyPI_Stats/internet.html'\n",
    "\n",
    "# 1) get the names from stats page\n",
    "\n",
    "names_internet = get_package_names(SEED_URL_internet, max_n = 2000)\n",
    "print(f\"Found {len(names_internet)} package names_internet on the page.\")\n",
    "\n",
    "# 2) enrich via PyPI JSON API\n",
    "\n",
    "rows = []\n",
    "for i, pkg in enumerate(names_internet, 1):\n",
    "    try:\n",
    "        rows.append(fetch_pypi_pkg(pkg))\n",
    "    except Exception as e:\n",
    "        print('skip', pkg, \"->\", e)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "\n",
    "product_internet = pd.DataFrame(rows)\n",
    "product_internet.dropna(subset = ['title'], inplace = True)\n",
    "product_internet['product_id'] = product_internet['product_id'].astype(str)\n",
    "product_internet['title'] = product_internet['title'].astype(str)\n",
    "product_internet['description']= product_internet['description'].fillna(\"\").astype(str)\n",
    "product_internet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1717f9d9-c93a-4dde-ab25-85b4125beaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192 package names_multimedia on the page.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pillow</td>\n",
       "      <td>pillow</td>\n",
       "      <td>Python Imaging Library (Fork)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emoji</td>\n",
       "      <td>emoji</td>\n",
       "      <td>Emoji for Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seaborn</td>\n",
       "      <td>seaborn</td>\n",
       "      <td>Statistical data visualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imagesize</td>\n",
       "      <td>imagesize</td>\n",
       "      <td>Getting image size from png/jpeg/jpeg2000/gif ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resampy</td>\n",
       "      <td>resampy</td>\n",
       "      <td>Efficient signal resampling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id      title                                        description\n",
       "0     pillow     pillow                      Python Imaging Library (Fork)\n",
       "1      emoji      emoji                                   Emoji for Python\n",
       "2    seaborn    seaborn                     Statistical data visualization\n",
       "3  imagesize  imagesize  Getting image size from png/jpeg/jpeg2000/gif ...\n",
       "4    resampy    resampy                        Efficient signal resampling"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED_URL_multimedia = 'https://n0x5.github.io/PyPI_Stats/multimedia.html'\n",
    "\n",
    "# 1) get the names from stats page\n",
    "\n",
    "names_multimedia = get_package_names(SEED_URL_multimedia, max_n = 500)\n",
    "print(f\"Found {len(names_multimedia)} package names_multimedia on the page.\")\n",
    "\n",
    "# 2) enrich via PyPI JSON API\n",
    "\n",
    "rows = []\n",
    "for i, pkg in enumerate(names_multimedia, 1):\n",
    "    try:\n",
    "        rows.append(fetch_pypi_pkg(pkg))\n",
    "    except Exception as e:\n",
    "        print('skip', pkg, \"->\", e)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "\n",
    "product_multimedia = pd.DataFrame(rows)\n",
    "product_multimedia.dropna(subset = ['title'], inplace = True)\n",
    "product_multimedia['product_id'] = product_multimedia['product_id'].astype(str)\n",
    "product_multimedia['title'] = product_multimedia['title'].astype(str)\n",
    "product_multimedia['description']= product_multimedia['description'].fillna(\"\").astype(str)\n",
    "product_multimedia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7397be14-4f80-49ea-bcc7-520bac425ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(product_internet.columns) == set(product_multimedia.columns), \"Mismatched coolumns!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4c7c14-cc24-4faa-8c17-40cd9dee42ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1221 entries, 0 to 1249\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   product_id   1221 non-null   object\n",
      " 1   title        1221 non-null   object\n",
      " 2   description  1221 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 38.2+ KB\n"
     ]
    }
   ],
   "source": [
    "product = pd.concat([product_internet, product_multimedia], ignore_index = True).drop_duplicates()\n",
    "product.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6324636-8a95-4355-9351-0741d734cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: ['numpy', 'matplotlib', 'scipy', 'scikit-learn']\n"
     ]
    }
   ],
   "source": [
    "# Ensure some must-have packages are present, then save\n",
    "\n",
    "MUST_HAVE = [\n",
    "    \"numpy\",\"pandas\",\"matplotlib\",\"scipy\",\"scikit-learn\",\"seaborn\",\"pytest\",\n",
    "    \"flask\",\"django\",\"fastapi\",\"pydantic\",\"sqlalchemy\",\"requests\",\"uvicorn\"\n",
    "]\n",
    "\n",
    "def _canon_pkg(s: str) -> str:\n",
    "    s = str(s or \"\").strip().lower().replace(\"_\",\"-\")\n",
    "    return re.sub(r\"[^a-z0-9\\-.]+\",\"\", s)\n",
    "\n",
    "def has_pkg(df, pkg):\n",
    "    canon = _canon_pkg(pkg)\n",
    "    by_id = df[\"product_id\"].astype(str).str.lower().str.replace(\"_\",\"-\", regex=False).eq(canon).any()\n",
    "    by_title = df[\"title\"].astype(str).str.lower().str.replace(\"_\",\"-\", regex=False).str.contains(rf\"\\b{re.escape(canon)}\\b\").any()\n",
    "    return bool(by_id or by_title)\n",
    "\n",
    "missing = [p for p in MUST_HAVE if not has_pkg(product, p)]\n",
    "rows = []\n",
    "for pkg in missing:\n",
    "    try:\n",
    "        rows.append(fetch_pypi_pkg(pkg)); time.sleep(0.2)\n",
    "    except Exception as e:\n",
    "        print(\"skip\", pkg, \"->\", e)\n",
    "\n",
    "if rows:\n",
    "    product = pd.concat([product, pd.DataFrame(rows)], ignore_index=True)\n",
    "    product.drop_duplicates(\"product_id\", keep=\"first\", inplace=True)\n",
    "    print(\"Added:\", [r[\"product_id\"] for r in rows])\n",
    "else:\n",
    "    print(\"All must-have packages already present.\")\n",
    "\n",
    "product.to_csv(\"products_pypi.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a4de8fb-7d24-451e-a7a1-d771e765ae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numpy present? True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>numpy</td>\n",
       "      <td>numpy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id  title\n",
       "1221      numpy  numpy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "matplotlib present? True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>matplotlib</td>\n",
       "      <td>matplotlib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id       title\n",
       "1222  matplotlib  matplotlib"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scipy present? True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>scipy</td>\n",
       "      <td>scipy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id  title\n",
       "1223      scipy  scipy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scikit-learn present? True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>scikit-learn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id         title\n",
       "1224  scikit-learn  scikit-learn"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "requests present? True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>requests</td>\n",
       "      <td>requests</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id     title\n",
       "1   requests  requests"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "for name in [\"numpy\",\"matplotlib\",\"scipy\",\"scikit-learn\",\"requests\"]:\n",
    "    hits = product[product[\"product_id\"].str.lower().eq(name)][[\"product_id\",\"title\"]].head(3)\n",
    "    print(f\"\\n{name} present? {not hits.empty}\")\n",
    "    display(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529f731-97d8-4572-8170-9ab57d009255",
   "metadata": {},
   "source": [
    "## Load Python FAQ Dataset & text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a0ac6-2a20-4448-a0dd-190060e8606a",
   "metadata": {},
   "source": [
    "#### Load FAQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f962e2bd-727b-4a01-b927-44d207167c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Python FAQ Dataset.csv from ZIP with encoding cp1252; rows=138\n",
      "Columns: ['Questions', 'Answers']\n"
     ]
    }
   ],
   "source": [
    "# Robust FAQ csv / zip loader (handles, cp1252 / latin1, comma / tsv)\n",
    "\n",
    "faq_path = \"Python FAQ Dataset.zip\"  \n",
    "\n",
    "if faq_path.lower().endswith(\".zip\"):\n",
    "    with zipfile.ZipFile(faq_path) as z:\n",
    "        names = z.namelist()\n",
    "        # pick the first csv/tsv-ish file\n",
    "        inner = next((n for n in names if n.lower().endswith((\".csv\",\".tsv\",\".txt\"))), names[0])\n",
    "        raw = z.read(inner)\n",
    "        last_err = None\n",
    "        for enc in [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\",\"iso-8859-1\"]:\n",
    "            try:\n",
    "                data = io.BytesIO(raw.decode(enc).encode(\"utf-8\"))\n",
    "                faq_raw = pd.read_csv(data, sep=None, engine=\"python\", on_bad_lines=\"skip\")\n",
    "                print(f\"Loaded {inner} from ZIP with encoding {enc}; rows={len(faq_raw)}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        else:\n",
    "            raise last_err\n",
    "else:\n",
    "    last_err = None\n",
    "    for enc in [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\",\"iso-8859-1\"]:\n",
    "        try:\n",
    "            faq_raw = pd.read_csv(faq_path, encoding=enc, sep=None, engine=\"python\", on_bad_lines=\"skip\")\n",
    "            print(f\"Loaded {faq_path} with encoding {enc}; rows={len(faq_raw)}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    else:\n",
    "        raise last_err\n",
    "faq_raw.head()\n",
    "print(\"Columns:\", list(faq_raw.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfe9794-368c-4139-a551-eaf31b6531c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ pairs: 121\n"
     ]
    }
   ],
   "source": [
    "lower_map = {c.lower().strip(): c for c in faq_raw.columns}\n",
    "CAND_Q = [\"question\",\"questions\",\"q\",\"prompt\",\"ask\"]\n",
    "CAND_A = [\"answer\",\"answers\",\"a\",\"response\",\"reply\"]\n",
    "\n",
    "def pick(colmap, cands):\n",
    "    for c in cands:\n",
    "        if c in colmap:\n",
    "            return colmap[c]\n",
    "    return None\n",
    "\n",
    "cq = pick(lower_map, CAND_Q)\n",
    "ca = pick(lower_map, CAND_A)\n",
    "\n",
    "if cq and ca:\n",
    "    Q = faq_raw[cq].astype(str)\n",
    "    A = faq_raw[ca].astype(str)\n",
    "else:\n",
    "    # fallback: first two non-empty columns\n",
    "    nonempty = [c for c in faq_raw.columns if faq_raw[c].notna().any()]\n",
    "    if len(nonempty) < 2:\n",
    "        raise ValueError(\"Could not find two non-empty columns for FAQ.\")\n",
    "    Q = faq_raw[nonempty[0]].astype(str)\n",
    "    A = faq_raw[nonempty[1]].astype(str)\n",
    "    print(f\"WARNING: fell back to columns: {nonempty[:2]}\")\n",
    "\n",
    "# --- clean, drop empties/dupes ---\n",
    "Q = Q.fillna(\"\").str.strip()\n",
    "A = A.fillna(\"\").str.strip()\n",
    "mask = (Q != \"\") & (A != \"\")\n",
    "Q = Q[mask]; A = A[mask]\n",
    "faq_df = pd.DataFrame({\"Q\": Q.values, \"A\": A.values}).drop_duplicates(\"Q\")\n",
    "faq_pairs = list(faq_df.itertuples(index=False, name=None))  # list[(Q,A)]\n",
    "print(f\"FAQ pairs: {len(faq_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf10d70-ef88-4c07-b41a-095313bf67c8",
   "metadata": {},
   "source": [
    "#### English preprocessing (spaCy lemmatization + stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f48a750-fdf8-4069-b9b4-c51361f56bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy enabled preprocessing\n"
     ]
    }
   ],
   "source": [
    "# English preprocessing (spaCy lemmatization if available, otherwise simple)\n",
    "\n",
    "WH_KEEP = {\"why\",\"what\",\"how\",\"where\",\"when\",\"which\",\"who\",\"whom\",\"whose\"}\n",
    "WORD_RE = re.compile(r\"[A-Za-z0-9][A-Za-z0-9._\\-]*\")  # keep digits/dot/underscore/hyphen\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"ner\",\"textcat\",\"senter\"])\n",
    "    STOP = set(nlp.Defaults.stop_words) - WH_KEEP\n",
    "\n",
    "    def preprocess_en(text: str):\n",
    "        toks = WORD_RE.findall(str(text).lower())\n",
    "        if not toks:\n",
    "            return []\n",
    "        doc = nlp(\" \".join(toks))  # tagger is enabled → no W108\n",
    "        out = []\n",
    "        for t in doc:\n",
    "            lem = t.lemma_.strip()\n",
    "            if not lem or lem in STOP:\n",
    "                continue\n",
    "            out.append(lem)\n",
    "        return out\n",
    "\n",
    "    print(\"spaCy enabled preprocessing\")\n",
    "\n",
    "except Exception:\n",
    "    # fallback tokenizer/stopwords\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    STOP = set(ENGLISH_STOP_WORDS) - WH_KEEP\n",
    "\n",
    "    def preprocess_en(text: str):\n",
    "        toks = WORD_RE.findall(str(text).lower())\n",
    "        return [t for t in toks if t and t not in STOP]\n",
    "\n",
    "    print(\"spaCy unavailable -> using simple tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdedc8f-a313-423a-af0d-df4907cc3738",
   "metadata": {},
   "source": [
    "## Train classifier (product vs chatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc64444-45a0-4422-add6-0fa6fecead2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urllib3</td>\n",
       "      <td>urllib3</td>\n",
       "      <td>HTTP library with thread-safe connection pooli...</td>\n",
       "      <td>urllib3 HTTP library with thread-safe connecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>requests</td>\n",
       "      <td>requests</td>\n",
       "      <td>Python HTTP for Humans.</td>\n",
       "      <td>requests Python HTTP for Humans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idna</td>\n",
       "      <td>idna</td>\n",
       "      <td>Internationalized Domain Names in Applications...</td>\n",
       "      <td>idna Internationalized Domain Names in Applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google-api-core</td>\n",
       "      <td>google-api-core</td>\n",
       "      <td>Google API client core library</td>\n",
       "      <td>google-api-core Google API client core library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google-auth</td>\n",
       "      <td>google-auth</td>\n",
       "      <td>Google Authentication Library</td>\n",
       "      <td>google-auth Google Authentication Library</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id            title  \\\n",
       "0          urllib3          urllib3   \n",
       "1         requests         requests   \n",
       "2             idna             idna   \n",
       "3  google-api-core  google-api-core   \n",
       "4      google-auth      google-auth   \n",
       "\n",
       "                                         description  \\\n",
       "0  HTTP library with thread-safe connection pooli...   \n",
       "1                            Python HTTP for Humans.   \n",
       "2  Internationalized Domain Names in Applications...   \n",
       "3                     Google API client core library   \n",
       "4                      Google Authentication Library   \n",
       "\n",
       "                                          text_clean  \n",
       "0  urllib3 HTTP library with thread-safe connecti...  \n",
       "1                   requests Python HTTP for Humans.  \n",
       "2  idna Internationalized Domain Names in Applica...  \n",
       "3     google-api-core Google API client core library  \n",
       "4          google-auth Google Authentication Library  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build labeled data\n",
    "\n",
    "product = pd.read_csv('products_pypi.csv')\n",
    "product['text_clean'] = (product.title.fillna(\"\") + \" \" + product.description.fillna(\"\"))\n",
    "\n",
    "product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359e9361-4268-4ada-9b81-acc6785a6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact overlap after grouped split: 0\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n",
    "\n",
    "# synthesize short product queries (text, product_id)\n",
    "\n",
    "def synth_product_queries(df: pd.DataFrame, per_item = 6, seed = 42):\n",
    "    random.seed(seed)\n",
    "    T = [\n",
    "        \"what is {name}\", \"how to use {name}\", \"pip install {name}\",\n",
    "        \"{name} tutorial\", \"{name} examples\", \"{name} docs\", \"{name} vs {alt}\"\n",
    "    ]\n",
    "    names = df['product_id'].astype(str).tolist()\n",
    "    rows = []\n",
    "    for name in names:\n",
    "        alts = [n for n in names if n != names] or [name]\n",
    "        for _ in range(per_item):\n",
    "            tpl = random.choice(T)\n",
    "            alt = random.choice(alts)\n",
    "            q = tpl.format(name = name, alt = alt)\n",
    "            rows.append({'text' : q, 'y' : 1, 'group' : f'prod_{name}'})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Positive from PyPI products\n",
    "\n",
    "pos_df = synth_product_queries(product, per_item = 8)\n",
    "\n",
    "# Negative from FAQ only\n",
    "\n",
    "neg_q = [q for q, _ in faq_pairs]\n",
    "neg_df = pd.DataFrame({'text': neg_q, 'y' : 0})\n",
    "neg_df['group'] = neg_df['text'].map(lambda s: 'neg_' + hashlib.md5(norm_text(s).encode()).hexdigest()) # to avoid duplicates\n",
    "\n",
    "# Drop exact duplicates\n",
    "\n",
    "pos_df = pos_df.drop_duplicates('text').reset_index(drop = True)\n",
    "neg_df = neg_df.drop_duplicates('text').reset_index(drop = True)\n",
    "\n",
    "# Balance sizes\n",
    "\n",
    "m = min(len(pos_df), len(neg_df))\n",
    "pos_s = pos_df.sample(n = m, random_state = 42)\n",
    "neg_s = neg_df.sample(n = m, random_state = 42)\n",
    "\n",
    "data = pd.concat([pos_s, neg_s], ignore_index = True)\n",
    "X = data['text'].tolist()\n",
    "y = data['y'].values\n",
    "groups = data['group'].values\n",
    "\n",
    "# Grouped split: no leakage of product queries across folds\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "X_train = [X[i] for i in train_idx]; y_train = y[train_idx]\n",
    "X_val   = [X[i] for i in val_idx];   y_val   = y[val_idx]\n",
    "\n",
    "print('Exact overlap after grouped split:', len(set(X_train) & set(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa76d0e-6361-43ab-9a71-d1202c5b0873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy: 0.9388 F1: 0.9302\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        26\n",
      "           1       1.00      0.87      0.93        23\n",
      "\n",
      "    accuracy                           0.94        49\n",
      "   macro avg       0.95      0.93      0.94        49\n",
      "weighted avg       0.95      0.94      0.94        49\n",
      "\n",
      "\n",
      "LR accuracy: 0.9388 F1: 0.9302\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        26\n",
      "           1       1.00      0.87      0.93        23\n",
      "\n",
      "    accuracy                           0.94        49\n",
      "   macro avg       0.95      0.93      0.94        49\n",
      "weighted avg       0.95      0.94      0.94        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(\n",
    "    ngram_range = (1, 2),\n",
    "    min_df = 2,\n",
    "    max_df = 0.95,\n",
    "    stop_words = 'english',\n",
    "    token_pattern = r\"[A-Za-z]{2,}\",\n",
    "    lowercase = True,\n",
    "    strip_accents = 'unicode'\n",
    ")\n",
    "\n",
    "pipe_svc = Pipeline([\n",
    "    ('tfidf', vec),\n",
    "    ('clf', LinearSVC(random_state = 42))\n",
    "])\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "pred_svc = pipe_svc.predict(X_val)\n",
    "\n",
    "print('SVC accuracy:', round(accuracy_score(y_val, pred_svc), 4), 'F1:', round(f1_score(y_val, pred_svc), 4))\n",
    "print('\\n', classification_report(y_val, pred_svc))\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('tfidf', vec),\n",
    "    ('clf', LogisticRegression(max_iter = 1000, random_state = 42))\n",
    "])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pred_lr = pipe_lr.predict(X_val)\n",
    "\n",
    "print('\\nLR accuracy:', round(accuracy_score(y_val, pred_lr), 4), \"F1:\", round(f1_score(y_val, pred_lr), 4))\n",
    "print(\"\\n\", classification_report(y_val, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a476c9-9487-443c-8480-1552b3e0b429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/product_query_lr.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipe_svc, \"models/product_query_svc.pkl\")\n",
    "joblib.dump(pipe_lr,  \"models/product_query_lr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70a2ff01-88e6-495e-8573-3032943873d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept: 117\n"
     ]
    }
   ],
   "source": [
    "def clean_pair(q,a):\n",
    "    q = re.sub(r\"\\s+\",\" \", str(q)).strip()\n",
    "    a = re.sub(r\"\\s+\",\" \", str(a)).strip()\n",
    "    return q, a\n",
    "\n",
    "faq_pairs = [clean_pair(q,a) for q,a in faq_pairs]\n",
    "faq_pairs = [(q,a) for q,a in faq_pairs\n",
    "             if len(a) >= 40 and \"python software foundation\" not in a.lower()]\n",
    "\n",
    "vec = TfidfVectorizer(min_df=2, max_df=0.9).fit([a for _,a in faq_pairs])\n",
    "A = vec.transform([a for _,a in faq_pairs])\n",
    "sim = cosine_similarity(A)\n",
    "\n",
    "keep = []\n",
    "seen = set()\n",
    "for i in range(len(faq_pairs)):\n",
    "    if i in seen: continue\n",
    "    dup_idx = np.where(sim[i] > 0.85)[0]\n",
    "    for j in dup_idx: seen.add(j)\n",
    "    keep.append(i)\n",
    "\n",
    "faq_pairs = [faq_pairs[i] for i in keep]\n",
    "print(\"Kept:\", len(faq_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82eab5b-26d8-4990-8ed5-94647dc8c63b",
   "metadata": {},
   "source": [
    "## Train Word2Vec & build Annoy indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9b2d4d8-2144-4d92-bbac-708985669ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a lexical first stage (BM25) \n",
    "\n",
    "faq_q_tok_bm25 = [preprocess_en(q) for q,_ in faq_pairs]\n",
    "bm25 = BM25Okapi(faq_q_tok_bm25)\n",
    "\n",
    "def _faq_bm25_lookup(query, k=3, min_score=1.5):\n",
    "    toks = preprocess_en(query)\n",
    "    if not toks: return None\n",
    "    scores = bm25.get_scores(toks)\n",
    "    idx = int(np.argmax(scores))\n",
    "    if scores[idx] >= min_score:\n",
    "        return faq_pairs[idx][1]  # answer\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2e9a38-53b2-4347-b750-b83adf3c67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize products and FAQ\n",
    "\n",
    "prod_title_tok = product['title'].fillna(\"\").apply(preprocess_en).tolist()\n",
    "prod_desc_tok = product['description'].fillna(\"\").apply(preprocess_en).tolist()\n",
    "faq_q_tok = [preprocess_en(q) for q,_ in faq_pairs]\n",
    "\n",
    "# Train W2V on combined corpus for good coverage\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences = prod_title_tok + prod_desc_tok + faq_q_tok,\n",
    "    vector_size = 100, window = 5, min_count = 1, sg = 1,\n",
    "    negative = 10, workers = os.cpu_count(), epochs = 10, seed = 42\n",
    ")\n",
    "\n",
    "w2v.wv.fill_norms()\n",
    "D = w2v.vector_size\n",
    "\n",
    "def encode_tokens(tokens):\n",
    "    vec = np.zeros(D, dtype = np.float32); n = 0\n",
    "    for w in tokens:\n",
    "        if w in w2v.wv:\n",
    "            vec += w2v.wv.get_vector(w, norm = True); n += 1\n",
    "    if n == 0: return None\n",
    "    vec /= n; nrm = np.linalg.norm(vec)\n",
    "    if nrm == 0: return None\n",
    "    return (vec / nrm).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "849e50fc-57f6-42b1-92b0-a9a1cf0f6db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed products: 1225 | FAQ: 117\n"
     ]
    }
   ],
   "source": [
    "# Product Annoy\n",
    "\n",
    "prod_index = AnnoyIndex(D, 'angular'); prod_map = {}; k = 0\n",
    "for r in product.itertuples(index = False):\n",
    "    t = preprocess_en(getattr(r, 'title', \"\"))\n",
    "    d = preprocess_en(getattr(r, 'description', \"\"))\n",
    "    v = encode_tokens(t + d)\n",
    "    if v is None:\n",
    "        continue\n",
    "    prod_index.add_item(k, v)\n",
    "    prod_map[k] = {'product_id' : str(r.product_id), 'title' : str(r.title)}\n",
    "    k += 1\n",
    "\n",
    "prod_index.build(30)\n",
    "os.makedirs(\"models\", exist_ok = True)\n",
    "prod_index.save(\"models/product.ann\")\n",
    "with open('models/product_map.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(prod_map, f, ensure_ascii = False)\n",
    "\n",
    "\n",
    "# FAQ Annoy\n",
    "\n",
    "faq_index = AnnoyIndex(D, 'angular'); faq_map = {}; j = 0\n",
    "_kept_questions = []\n",
    "for q, a in faq_pairs:\n",
    "    v = encode_tokens(preprocess_en(q))\n",
    "    if v is None:\n",
    "        continue\n",
    "    faq_index.add_item(j, v)\n",
    "    faq_map[j] = a\n",
    "    _kept_questions.append(q)   # save only those actually indexed\n",
    "    j += 1\n",
    "\n",
    "faq_index.build(10)\n",
    "faq_index.save('models/faq.ann')\n",
    "with open('models/faq_map.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(faq_map, f, ensure_ascii = False)\n",
    "\n",
    "# Save the FAQ questions for future lexical matching in fresh sessions\n",
    "\n",
    "with open(\"models/faq_q.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(_kept_questions, f, ensure_ascii=False)\n",
    "    \n",
    "# Save vectors\n",
    "\n",
    "w2v.save(\"models/w2v_model.model\")\n",
    "w2v.wv.save(\"models/w2v_vectors.kv\")\n",
    "\n",
    "print(f\"Indexed products: {k} | FAQ: {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efff20-0cc8-4ebc-ad91-dbb8cb2aa518",
   "metadata": {},
   "source": [
    "## Final router: get_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f571b7-ebbb-4ac6-b446-ac02f8346047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load artifacts (when starting a new session)\n",
    "\n",
    "product = pd.read_csv(\"products_pypi.csv\")\n",
    "product[\"product_id\"] = product[\"product_id\"].astype(str)\n",
    "product[\"title\"] = product[\"title\"].fillna(\"\").astype(str)\n",
    "product[\"description\"] = product[\"description\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Load W2V to get dimension D\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec.load(\"models/w2v_model.model\")\n",
    "w2v.wv.fill_norms()\n",
    "D = w2v.vector_size\n",
    "\n",
    "def encode_tokens(tokens):\n",
    "    vec = np.zeros(D, dtype=np.float32); n = 0\n",
    "    for w in tokens:\n",
    "        if w in w2v.wv:\n",
    "            vec += w2v.wv.get_vector(w, norm=True); n += 1\n",
    "    if n == 0: return None\n",
    "    vec /= n; nrm = np.linalg.norm(vec)\n",
    "    if nrm == 0: return None\n",
    "    return (vec / nrm).astype(np.float32)\n",
    "\n",
    "svc = joblib.load(\"models/product_query_svc.pkl\")\n",
    "try:\n",
    "    lr  = joblib.load(\"models/product_query_lr.pkl\")\n",
    "except Exception:\n",
    "    lr = None\n",
    "\n",
    "prod_index = AnnoyIndex(D, \"angular\"); prod_index.load(\"models/product.ann\")\n",
    "faq_index  = AnnoyIndex(D, \"angular\"); faq_index.load(\"models/faq.ann\")\n",
    "\n",
    "with open(\"models/product_map.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    prod_map = {int(k): v for k, v in json.load(f).items()}\n",
    "with open(\"models/faq_map.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    faq_map  = {int(k): v for k, v in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d73cc75-2f22-4a08-aa16-ce464c538c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lexical index for FAQ questions\n",
    "\n",
    "def _encode_query(text): \n",
    "    return encode_tokens(preprocess_en(text))\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s or \"\").casefold()\n",
    "    s = re.sub(r\"[\\\"'’`´]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "FAQ_STOP = {\"package\",\"library\",\"module\",\"install\",\"pip\",\"pip3\",\"pipx\",\n",
    "            \"pypi\",\"on\",\"for\",\"with\",\"the\",\"a\",\"an\",\"in\",\"to\",\"of\",\"and\",\"is\",\"are\"}\n",
    "\n",
    "WH = {\"what\",\"why\",\"how\",\"where\",\"when\",\"which\",\"who\",\"whom\",\"whose\",\n",
    "      \"can\",\"are\",\"is\",\"does\",\"do\",\"did\",\"was\",\"were\",\"will\",\"shall\",\n",
    "      \"should\",\"could\",\"would\",\"may\",\"might\",\"must\"}\n",
    "\n",
    "GENERIC_Q = {\"use\",\"using\",\"today\",\"now\",\"way\",\"ways\",\"make\",\"get\",\"work\",\n",
    "             \"works\",\"find\",\"need\",\"want\",\"example\",\"examples\",\"show\",\"tell\",\"help\"}\n",
    "\n",
    "FAQ_OVERLAP_STOP = FAQ_STOP | WH | GENERIC_Q\n",
    "\n",
    "# Try to get questions from memory (faq_pairs) or fallback to saved file\n",
    "\n",
    "_faq_questions = []\n",
    "try:\n",
    "    _faq_questions = [q for q,_ in faq_pairs]\n",
    "except Exception:\n",
    "    try:\n",
    "        with open(\"models/faq_q.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "            _faq_questions = json.load(f)\n",
    "    except Exception:\n",
    "        _faq_questions = []\n",
    "\n",
    "faq_lex = []\n",
    "if _faq_questions:\n",
    "    for i, q in enumerate(_faq_questions):\n",
    "        a = faq_map.get(i, \"\")\n",
    "        nq = _norm(q)\n",
    "        # tokens for exact match (your previous logic)\n",
    "        toks_exact = set(t for t in re.findall(r\"[a-z0-9]+\", nq) if t not in FAQ_STOP)\n",
    "        # stricter tokens for overlap checks (drops how/use/etc.)\n",
    "        toks_overlap = set(t for t in re.findall(r\"[a-z0-9]+\", nq) if t not in FAQ_OVERLAP_STOP)\n",
    "        faq_lex.append({\"nq\": nq, \"toks\": toks_exact, \"toks_overlap\": toks_overlap, \"ans\": a})\n",
    "\n",
    "\n",
    "def _faq_lexical_lookup(query: str, min_overlap: int = 2):\n",
    "    if not faq_lex:\n",
    "        return None\n",
    "    qn = _norm(query)\n",
    "\n",
    "    # exact match (uses broader tokens logic)\n",
    "    for rec in faq_lex:\n",
    "        if rec[\"nq\"] == qn:\n",
    "            return rec[\"ans\"]\n",
    "\n",
    "    # overlap on stricter tokens\n",
    "    qtok = set(t for t in re.findall(r\"[a-z0-9]+\", qn) if t not in FAQ_OVERLAP_STOP)\n",
    "    best, best_ov = None, 0\n",
    "    for rec in faq_lex:\n",
    "        ov = len(qtok & rec[\"toks_overlap\"])\n",
    "        if ov > best_ov:\n",
    "            best, best_ov = rec, ov\n",
    "    if best and best_ov >= min_overlap:\n",
    "        return best[\"ans\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6065f258-345e-427e-a474-d8132335a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: why was python created in the first place?\n",
      "FAQ lexical: Here’s a very brief summary of what started it all, written by Guido van Rossum: I had extensive experience with implementing an interpreted language in the ABC group at CWI, and from working with this group I had learned a lot about language design. This is the origin of many Python features, including the use of indentation for statement grouping and the inclusion of very-high-level data types (although the details are all different in Python). I had a number of gripes about the ABC language, but also liked many of its features. It was impossible to extend the ABC language (or its implementation) to remedy my complaints – in fact its lack of extensibility was one of its biggest problems. I had some experience with using Modula-2+ and talked with the designers of Modula-3 and read the Modula-3 report. Modula-3 is the origin of the syntax and semantics used for exceptions, and some other Python features. I was working in the Amoeba distributed operating system group at CWI. We needed a better way to do system administration than by writing either C programs or Bourne shell scripts, since Amoeba had its own system call interface which wasn’t easily accessible from the Bourne shell. My experience with error handling in Amoeba made me acutely aware of the importance of exceptions as a programming language feature. It occurred to me that a scripting language with a syntax like ABC but with access to the Amoeba system calls would fill the need. I realized that it would be foolish to write an Amoeba-specific language, so I decided that I needed a language that was generally extensible. During the 1989 Christmas holidays, I had a lot of time on my hand, so I decided to give it a try. During the next year, while still mostly working on it in my own time, Python was used in the Amoeba project with increasing success, and the feedback from colleagues made me add many early improvements. In February 1991, after just over a year of development, I decided to post to USENET. The rest is in the Misc/HISTORY file.\n",
      "FAQ best sim: 1.0\n",
      "PROD best sim: 0.996\n",
      "\n",
      "Q: are there any books on python?\n",
      "FAQ lexical: Yes, there are many, and more are being published. See the python.org wiki at https://wiki.python.org/moin/PythonBooks for a list. You can also search online bookstores for “Python” and filter out the Monty Python references; or perhaps search for “Python” and “language”.\n",
      "FAQ best sim: 0.747\n",
      "PROD best sim: 0.737\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check\n",
    "\n",
    "def _diag(q):\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"FAQ lexical:\", _faq_lexical_lookup(q, min_overlap=3))\n",
    "    qv = _encode_query(q)\n",
    "    if qv is None:\n",
    "        print(\"qv: None\")\n",
    "        return\n",
    "    # best FAQ/Product similarities (Annoy angular -> cosine-like)\n",
    "    ids_f, dists_f = faq_index.get_nns_by_vector(qv, 3, include_distances=True)\n",
    "    ids_p, dists_p = prod_index.get_nns_by_vector(qv, 3, include_distances=True)\n",
    "    fs = [1 - (d*d)/2.0 for d in dists_f]\n",
    "    ps = [1 - (d*d)/2.0 for d in dists_p]\n",
    "    print(\"FAQ best sim:\", round(fs[0],3) if fs else None)\n",
    "    print(\"PROD best sim:\", round(ps[0],3) if ps else None)\n",
    "\n",
    "_diag(\"why was python created in the first place?\")\n",
    "_diag(\"are there any books on python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "866faa1c-ebc6-4598-9043-e7840e9b3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name handling for products (kept here so it runs after product load)\n",
    "\n",
    "def _canon_pkg(s: str) -> str:\n",
    "    s = str(s or \"\").strip().lower().replace(\"_\",\"-\")\n",
    "    return re.sub(r\"[^a-z0-9\\-.]+\", \"\", s)\n",
    "\n",
    "name_index = {}\n",
    "for r in product.itertuples(index=False):\n",
    "    pid   = _canon_pkg(getattr(r, \"product_id\", \"\"))\n",
    "    title = str(getattr(r, \"title\", \"\")) or pid\n",
    "    if not pid:\n",
    "        continue\n",
    "    rec = {\"product_id\": str(getattr(r, \"product_id\")), \"title\": title}\n",
    "    name_index[pid] = rec\n",
    "    alias = _canon_pkg(title)\n",
    "    if alias and alias not in name_index:\n",
    "        name_index[alias] = rec\n",
    "    if alias.startswith(\"python-\"):\n",
    "        core = alias[len(\"python-\"):]\n",
    "        if core and core not in name_index:\n",
    "            name_index[core] = rec\n",
    "LEX_STOP = {\n",
    "    \"python\",\"package\",\"library\",\"module\",\"install\",\"pip\",\"pip3\",\"pipx\",\"pypi\",\n",
    "    \"on\",\"for\",\"with\",\"the\",\"a\",\"an\",\"in\",\"to\",\"of\",\"and\",\"is\",\"are\"\n",
    "}\n",
    "\n",
    "def _name_hit(query: str):\n",
    "    q = str(query or \"\").lower()\n",
    "    m = re.search(r\"\\b(?:pip|pip3|pipx)\\s+install\\s+([a-z0-9._\\-]+)\\b\", q)\n",
    "    if not m:\n",
    "        return None\n",
    "    cand = _canon_pkg(m.group(1))\n",
    "    if cand in name_index:\n",
    "        rec = name_index[cand]\n",
    "        return f\"{rec['product_id']} {rec['title']}\".strip()\n",
    "    return None\n",
    "\n",
    "# Build product lexical index with token sets\n",
    "\n",
    "PRODUCT_TOKEN_RE = re.compile(r\"[a-z0-9]+\")\n",
    "\n",
    "def _tokset(text, stop = LEX_STOP):\n",
    "    return set(t for t in PRODUCT_TOKEN_RE.findall(_norm(text)) if t not in stop)\n",
    "    \n",
    "product_lex = []\n",
    "for r in product.itertuples(index=False):\n",
    "    title = str(getattr(r, \"title\", \"\"))\n",
    "    desc  = str(getattr(r, \"description\", \"\"))\n",
    "    product_lex.append({\n",
    "        \"product_id\": str(r.product_id),\n",
    "        \"title\": title,\n",
    "        \"norm_title\": _norm(title),\n",
    "        \"title_toks\": _tokset(title, stop = LEX_STOP),    # title tokens\n",
    "        \"desc_toks\":  _tokset(desc, stop = LEX_STOP),     # description tokens\n",
    "    })\n",
    "\n",
    "def _lexical_product_lookup(query: str):\n",
    "    qn = _norm(query)\n",
    "    if not qn:\n",
    "        return None\n",
    "\n",
    "    # exact title fast path\n",
    "    for rec in product_lex:\n",
    "        if rec[\"norm_title\"] == qn:\n",
    "            return f\"{rec['product_id']} {rec['title']}\".strip()\n",
    "\n",
    "    qtok = _tokset(qn, stop = LEX_STOP)\n",
    "    \n",
    "    best, best_title_overlap, best_total = None, 0, 0\n",
    "    for rec in product_lex:\n",
    "        ot = len(qtok & rec[\"title_toks\"])\n",
    "        od = len(qtok & rec[\"desc_toks\"])\n",
    "        total = ot + od\n",
    "        \n",
    "        # prefer more title overlap, then total overlap\n",
    "        if (ot, total) > (best_title_overlap, best_total):\n",
    "            best, best_title_overlap, best_total = rec, ot, total\n",
    "\n",
    "    # REQUIRE >= 2 title tokens overlapped (prevents single \"requests\" matches)\n",
    "    if best and best_title_overlap >= 2:\n",
    "        return f\"{best['product_id']} {best['title']}\".strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "123446bf-9f8b-4760-8f72-1a7b0b3c5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _best_sim_product(qv, k=3):\n",
    "    ids, dists = prod_index.get_nns_by_vector(qv, k, include_distances=True)\n",
    "    if not ids: \n",
    "        return None, -1.0\n",
    "    sims = [1.0 - (d*d)/2.0 for d in dists]  # Annoy angular -> cosine-like\n",
    "    return ids[0], sims[0]\n",
    "\n",
    "def _best_sim_faq(qv, k=3):\n",
    "    ids, dists = faq_index.get_nns_by_vector(qv, k, include_distances=True)\n",
    "    if not ids:\n",
    "        return None, -1.0\n",
    "    sims = [1.0 - (d*d)/2.0 for d in dists]\n",
    "    return ids[0], sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93887b3b-ba8d-4ae0-960d-e7c8233e39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x: float) -> float:\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def _score_positive(model, text: str):\n",
    "    \"\"\" Return P (product) if available, else sigmoid(decision_function),\n",
    "    else None if the model exposes neither. \"\"\"\n",
    "\n",
    "    if model is None:\n",
    "        return None\n",
    "    try:\n",
    "        return float(model.predict_proba([text])[0][1])\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        margin = float(model.decision_function([text])[0])\n",
    "        return _sigmoid(margin)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_product_query(text: str, svc_floor : float = 0.50, lr_floor :float = 0.60) -> bool:\n",
    "    \"\"\" Route-to-product gate:\n",
    "    1) SVC must predict class 1 (product)\n",
    "    2) Optionally require SVC confidence (if available)\n",
    "    3) Optionally require LR confidence (if available) \"\"\"\n",
    "\n",
    "    if not str(text).strip():\n",
    "        return False\n",
    "\n",
    "    # hard decision from SVC first\n",
    "    try:\n",
    "        svc_pred = int(svc.predict([text])[0]) == 1 if svc is not None else False\n",
    "    except Exception:\n",
    "        svc_pred = False\n",
    "    if not svc_pred:\n",
    "        return False\n",
    "\n",
    "    # soft check on SVC confidence\n",
    "    ps = _score_positive(svc, text)\n",
    "    if ps is not None and ps < svc_floor:\n",
    "        return False\n",
    "\n",
    "    # LR veto (if LR is present and exposes proba / margin)\n",
    "    plr = _score_positive(lr, text)\n",
    "    if plr is not None and plr < lr_floor:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bb0d6cd-04d5-434b-a660-dce127be8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy numpy\n",
      "None\n",
      "requests requests\n",
      "scikit-learn scikit-learn\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Quick self-test\n",
    "\n",
    "print(_name_hit(\"pip install numpy\"))            \n",
    "print(_name_hit(\"how to use requests?\"))         \n",
    "\n",
    "print(_lexical_product_lookup(\"requests\"))       \n",
    "print(_lexical_product_lookup(\"scikit learn\"))   \n",
    "print(_lexical_product_lookup(\"how to use requests?\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9333571-0453-462b-a6de-9ac6208735fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: query tokens (for FAQ domain) using your lighter FAQ_STOP\n",
    "\n",
    "def _qtoks_overlap(q: str):\n",
    "    return set(t for t in re.findall(r\"[a-z0-9]+\", _norm(q)) if t not in FAQ_OVERLAP_STOP)\n",
    "\n",
    "# Require semantic FAQ hits to also share some words with the FAQ question\n",
    "\n",
    "def _faq_semantic_ok(query: str, fid: int | None, fsim: float, min_overlap: int = 2) -> bool:\n",
    "    if fid is None or fsim < FAQ_SIM_TH:\n",
    "        return False\n",
    "    try:\n",
    "        cand = faq_lex[fid]\n",
    "    except Exception:\n",
    "        return False\n",
    "    return len(_qtoks_overlap(query) & cand[\"toks_overlap\"]) >= min_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f08802b2-01fe-499b-bc0b-75c2cab05c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preference thresholds\n",
    "\n",
    "PROD_SIM_TH = 0.60   # product must be fairly strong\n",
    "FAQ_SIM_TH  = 0.35   # FAQ can be a bit looser\n",
    "PREF_MARGIN = 0.10   # product must beat FAQ by this much to win\n",
    "\n",
    "_WH_WORDS = {\"what\",\"why\",\"how\",\"where\",\"when\",\"which\",\"who\",\"whom\",\"whose\",\"can\",\"are\",\"is\",\"does\",\"do\"}\n",
    "\n",
    "def _looks_like_question(q: str) -> bool:\n",
    "    s = (q or \"\").strip().lower()\n",
    "    if not s: return False\n",
    "    if s.endswith(\"?\"): return True\n",
    "    # starts with wh-word or auxiliary (\"can\", \"are\", \"is\", \"does\", \"do\")\n",
    "    first = re.findall(r\"^[a-z]+\", s)\n",
    "    return bool(first and first[0] in _WH_WORDS)\n",
    "\n",
    "def get_answer(query: str):\n",
    "    # 0) explicit package name / pip install\n",
    "    hit = _name_hit(query)\n",
    "    if hit:\n",
    "        return hit\n",
    "\n",
    "    # 1) FAQ lexical FIRST (slightly looser: 2)\n",
    "    hit = _faq_lexical_lookup(query, min_overlap=2)\n",
    "    if hit:\n",
    "        return hit\n",
    "\n",
    "    # 2) product lexical (strict)\n",
    "    hit = _lexical_product_lookup(query)\n",
    "    if hit:\n",
    "        return hit\n",
    "\n",
    "    # 3) Encode once and compare sims\n",
    "    qv = _encode_query(query)\n",
    "    if qv is None:\n",
    "        return \"Sorry, I didn't understand.\"\n",
    "\n",
    "    pid, psim = _best_sim_product(qv, k=3)\n",
    "    fid, fsim = _best_sim_faq(qv, k=3)\n",
    "\n",
    "    # 3a) If it looks like a QUESTION, bias toward FAQ — but require lexical overlap too\n",
    "    if _looks_like_question(query):\n",
    "        if _faq_semantic_ok(query, fid, fsim, min_overlap=2) and \\\n",
    "           (fsim + PREF_MARGIN/2 >= psim or psim < PROD_SIM_TH + 0.05):\n",
    "            return faq_map[fid] if fid is not None else \"Sorry, I couldn't find an answer.\"\n",
    "\n",
    "    # 3b) Otherwise, prefer FAQ unless product is clearly stronger — and require overlap\n",
    "    if _faq_semantic_ok(query, fid, fsim, min_overlap=2) and \\\n",
    "       (fsim + PREF_MARGIN >= psim or psim < PROD_SIM_TH):\n",
    "        return faq_map[fid] if fid is not None else \"Sorry, I couldn't find an answer.\"\n",
    "\n",
    "    # Allow product only if strong and clearly better\n",
    "    if psim >= PROD_SIM_TH and (psim >= fsim + PREF_MARGIN):\n",
    "        item = prod_map[pid]\n",
    "        return f\"{item['product_id']} {item['title']}\".strip()\n",
    "\n",
    "    # 4) Tie-breaker via classifier (still conservative)\n",
    "    if is_product_query(query, svc_floor=0.55, lr_floor=0.65):\n",
    "        if psim >= PROD_SIM_TH - 0.05 and (psim >= fsim + PREF_MARGIN/2):\n",
    "            item = prod_map[pid]\n",
    "            return f\"{item['product_id']} {item['title']}\".strip()\n",
    "\n",
    "    # 5) Default to FAQ only if semantic hit also passes overlap; else fallback\n",
    "    if _faq_semantic_ok(query, fid, fsim, min_overlap=2):\n",
    "        return faq_map[fid]\n",
    "    return \"Sorry, I couldn't find an answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24cd35fc-5330-44b5-b99f-de8635b28e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy numpy\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('pip install numpy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c7647b7-6488-4f66-bab9-6a982d18f9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are many, and more are being published. See the python.org wiki at https://wiki.python.org/moin/PythonBooks for a list. You can also search online bookstores for “Python” and filter out the Monty Python references; or perhaps search for “Python” and “language”.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"are there any books on python?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5210398-3060-4de4-9352-200272e1fd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn scikit-learn\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"scikit learn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d532aa9d-e396-4921-b5a3-9b9cd8cee195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I couldn't find an answer.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('what is the weather today'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0236670b-7b5a-41ee-85a3-c3fc2f0a9497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I couldn't find an answer.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"how to use requests?\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b7e1063-33ff-41dc-ad7e-a713c337e48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I couldn't find an answer.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"install package on windows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bee918-7133-43e0-b057-f98295a88490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
